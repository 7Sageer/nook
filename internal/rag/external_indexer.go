package rag

import (
	"fmt"
	"os"
	"path/filepath"
	"strings"
	"time"

	"notion-lite/internal/document"
	"notion-lite/internal/fileextract"
	"notion-lite/internal/opengraph"
)

// ExternalIndexer handles indexing of external content (bookmarks and files)
type ExternalIndexer struct {
	store      *VectorStore
	embedder   EmbeddingClient
	docRepo    *document.Repository
	docStorage *document.Storage
	indexer    *Indexer
	dataPath   string
}

// NewExternalIndexer creates a new external content indexer
func NewExternalIndexer(
	store *VectorStore,
	embedder EmbeddingClient,
	docRepo *document.Repository,
	docStorage *document.Storage,
	indexer *Indexer,
	dataPath string,
) *ExternalIndexer {
	return &ExternalIndexer{
		store:      store,
		embedder:   embedder,
		docRepo:    docRepo,
		docStorage: docStorage,
		indexer:    indexer,
		dataPath:   dataPath,
	}
}

// IndexBookmarkContent ç´¢å¼•ä¹¦ç­¾ç½‘é¡µå†…å®¹ï¼ˆåˆ†å—å­˜å‚¨ï¼‰
func (e *ExternalIndexer) IndexBookmarkContent(url, sourceDocID, blockID string) error {
	// 1. æŠ“å–ç½‘é¡µå†…å®¹
	content, err := opengraph.FetchContent(url)
	if err != nil {
		return fmt.Errorf("failed to fetch content: %w", err)
	}

	// 2. æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºç©º
	if content.TextContent == "" {
		return fmt.Errorf("no content extracted from URL")
	}

	// 3. æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
	headingContext := content.Title
	if content.SiteName != "" {
		headingContext = fmt.Sprintf("%s - %s", content.Title, content.SiteName)
	}

	// 4. ç”ŸæˆåŸºç¡€ ID
	baseID := fmt.Sprintf("%s_%s_bookmark", sourceDocID, blockID)

	// 5. åˆ é™¤è¯¥ bookmark block çš„æ—§ chunksï¼ˆä¿®å¤é‡æ–°ç´¢å¼•æ—¶çš„ä¸»é”®å†²çªï¼‰
	if err := e.store.DeleteBlocksByPrefix(baseID); err != nil {
		fmt.Printf("âš ï¸ [RAG] Failed to delete old bookmark chunks for %s: %v\n", baseID, err)
	}

	// 5.1 ä¿å­˜å®Œæ•´æå–å†…å®¹ï¼ˆä¾› MCP å·¥å…·è¯»å–ï¼‰
	if err := e.store.SaveExternalContent(&ExternalBlockContent{
		ID:          fmt.Sprintf("%s_%s", sourceDocID, blockID),
		DocID:       sourceDocID,
		BlockID:     blockID,
		BlockType:   "bookmark",
		URL:         url,
		Title:       content.Title,
		RawContent:  content.TextContent,
		ExtractedAt: time.Now().Unix(),
	}); err != nil {
		fmt.Printf("âš ï¸ [RAG] Failed to save bookmark content for %s: %v\n", baseID, err)
	}

	// 6. å¯¹å†…å®¹è¿›è¡Œåˆ†å—
	chunks := ChunkTextContent(content.TextContent, headingContext, baseID, e.indexer.chunkConfig)

	// å¦‚æœåˆ†å—ç»“æœä¸ºç©ºï¼Œåˆ›å»ºä¸€ä¸ªå•ç‹¬çš„å—
	if len(chunks) == 0 {
		chunks = []ExtractedBlock{{
			ID:             baseID,
			Type:           "bookmark",
			Content:        content.TextContent,
			HeadingContext: headingContext,
		}}
	}

	// è°ƒè¯•è¾“å‡º
	if debugChunks {
		fmt.Printf("\nğŸ”– [RAG] Indexing bookmark: %s\n", url)
		fmt.Printf("   Title: %s\n", content.Title)
		fmt.Printf("   Total chunks: %d\n", len(chunks))
		fmt.Println("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
		for i, chunk := range chunks {
			fmt.Printf("   [%d] ID: %s\n", i, chunk.ID)
			fmt.Printf("       Content (%4d chars): %s\n",
				len(chunk.Content), truncateContent(chunk.Content, 80))
		}
		fmt.Println("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
	}

	// 7. ä¸ºæ¯ä¸ª chunk ç”Ÿæˆ embedding å¹¶å­˜å‚¨
	successCount := 0
	failedCount := 0
	var lastError error
	for _, chunk := range chunks {
		if chunk.Content == "" {
			continue
		}

		embedding, err := e.embedder.Embed(chunk.Content)
		if err != nil {
			failedCount++
			lastError = err
			fmt.Printf("âš ï¸ [RAG] Failed to embed bookmark chunk %s: %v\n", chunk.ID, err)
			continue // è·³è¿‡å¤±è´¥çš„å—
		}

		contentHash := HashContent(chunk.Content)
		if err := e.store.Upsert(&BlockVector{
			ID:             chunk.ID,
			SourceBlockID:  blockID, // BookmarkBlock çš„ BlockNote IDï¼Œç”¨äºå®šä½
			DocID:          sourceDocID,
			Content:        chunk.Content,
			ContentHash:    contentHash,
			BlockType:      "bookmark",
			HeadingContext: chunk.HeadingContext,
			Embedding:      embedding,
		}); err != nil {
			fmt.Printf("âš ï¸ [RAG] Failed to upsert bookmark chunk %s: %v\n", chunk.ID, err)
			failedCount++
		} else {
			successCount++
		}
	}

	// å¦‚æœæ‰€æœ‰ chunks éƒ½åµŒå…¥å¤±è´¥ï¼Œè¿”å›é”™è¯¯
	if successCount == 0 && failedCount > 0 {
		return fmt.Errorf("embedding failed: %v", lastError)
	}

	return nil
}

// IndexFileContent ç´¢å¼•æ–‡ä»¶å†…å®¹ï¼ˆåˆ†å—å­˜å‚¨ï¼‰
func (e *ExternalIndexer) IndexFileContent(filePath, sourceDocID, blockID string) error {
	// 1. è·å–å®Œæ•´æ–‡ä»¶è·¯å¾„
	fullPath := filepath.Join(e.dataPath, strings.TrimPrefix(filePath, "/"))

	// 2. æå–æ–‡æœ¬å†…å®¹
	textContent, err := fileextract.ExtractText(fullPath)
	if err != nil {
		return fmt.Errorf("failed to extract text: %w", err)
	}

	if textContent == "" {
		return fmt.Errorf("no text content extracted from file")
	}

	// 3. æ„å»ºä¸Šä¸‹æ–‡ï¼ˆä½¿ç”¨æ–‡ä»¶åï¼‰
	fileName := filepath.Base(fullPath)
	headingContext := fileName

	// 4. ç”ŸæˆåŸºç¡€ ID
	baseID := fmt.Sprintf("%s_%s_file", sourceDocID, blockID)

	// 5. åˆ é™¤è¯¥ file block çš„æ—§ chunksï¼ˆä¿®å¤é‡æ–°ç´¢å¼•æ—¶çš„ä¸»é”®å†²çªï¼‰
	if err := e.store.DeleteBlocksByPrefix(baseID); err != nil {
		fmt.Printf("âš ï¸ [RAG] Failed to delete old file chunks for %s: %v\n", baseID, err)
	}

	// 5.1 ä¿å­˜å®Œæ•´æå–å†…å®¹ï¼ˆä¾› MCP å·¥å…·è¯»å–ï¼‰
	if err := e.store.SaveExternalContent(&ExternalBlockContent{
		ID:          fmt.Sprintf("%s_%s", sourceDocID, blockID),
		DocID:       sourceDocID,
		BlockID:     blockID,
		BlockType:   "file",
		FilePath:    filePath,
		Title:       fileName,
		RawContent:  textContent,
		ExtractedAt: time.Now().Unix(),
	}); err != nil {
		fmt.Printf("âš ï¸ [RAG] Failed to save file content for %s: %v\n", baseID, err)
	}

	// 6. å¯¹å†…å®¹è¿›è¡Œåˆ†å—
	chunks := ChunkTextContent(textContent, headingContext, baseID, e.indexer.chunkConfig)

	// å¦‚æœåˆ†å—ç»“æœä¸ºç©ºï¼Œåˆ›å»ºä¸€ä¸ªå•ç‹¬çš„å—
	if len(chunks) == 0 {
		chunks = []ExtractedBlock{{
			ID:             baseID,
			Type:           "file",
			Content:        textContent,
			HeadingContext: headingContext,
		}}
	}

	// è°ƒè¯•è¾“å‡º
	if debugChunks {
		fmt.Printf("\nğŸ“„ [RAG] Indexing file: %s\n", fileName)
		fmt.Printf("   Total chunks: %d\n", len(chunks))
		fmt.Println("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
		for i, chunk := range chunks {
			fmt.Printf("   [%d] ID: %s\n", i, chunk.ID)
			fmt.Printf("       Content (%4d chars): %s\n",
				len(chunk.Content), truncateContent(chunk.Content, 80))
		}
		fmt.Println("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
	}

	// 7. ä¸ºæ¯ä¸ª chunk ç”Ÿæˆ embedding å¹¶å­˜å‚¨
	successCount := 0
	failedCount := 0
	var lastError error
	for _, chunk := range chunks {
		if chunk.Content == "" {
			continue
		}

		embedding, err := e.embedder.Embed(chunk.Content)
		if err != nil {
			failedCount++
			lastError = err
			fmt.Printf("âš ï¸ [RAG] Failed to embed file chunk %s: %v\n", chunk.ID, err)
			continue // è·³è¿‡å¤±è´¥çš„å—
		}

		contentHash := HashContent(chunk.Content)
		if err := e.store.Upsert(&BlockVector{
			ID:             chunk.ID,
			SourceBlockID:  blockID, // FileBlock çš„ BlockNote IDï¼Œç”¨äºå®šä½
			DocID:          sourceDocID,
			Content:        chunk.Content,
			ContentHash:    contentHash,
			BlockType:      "file",
			HeadingContext: chunk.HeadingContext,
			FilePath:       filePath, // å­˜å‚¨æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºåˆ é™¤æ—¶æ¸…ç†ç‰©ç†æ–‡ä»¶
			Embedding:      embedding,
		}); err != nil {
			fmt.Printf("âŒ [RAG] Failed to upsert file chunk %s: %v\n", chunk.ID, err)
			failedCount++
		} else {
			successCount++
			if debugChunks {
				fmt.Printf("âœ… [RAG] Stored file chunk: %s\n", chunk.ID)
			}
		}
	}

	// å¦‚æœæ‰€æœ‰ chunks éƒ½åµŒå…¥å¤±è´¥ï¼Œè¿”å›é”™è¯¯
	if successCount == 0 && failedCount > 0 {
		return fmt.Errorf("embedding failed: %v", lastError)
	}

	return nil
}

// FolderIndexResult æ–‡ä»¶å¤¹ç´¢å¼•ç»“æœ
type FolderIndexResult struct {
	TotalFiles   int      `json:"totalFiles"`
	SuccessCount int      `json:"successCount"`
	FailedCount  int      `json:"failedCount"`
	FailedFiles  []string `json:"failedFiles"`
}

// supportedExtensions æ”¯æŒç´¢å¼•çš„æ–‡ä»¶æ‰©å±•å
var supportedExtensions = map[string]bool{
	".pdf":  true,
	".docx": true,
	".xlsx": true,
	".epub": true,
	".html": true,
	".htm":  true,
	".txt":  true,
	".md":   true,
}

// IndexFolderContent ç´¢å¼•æ–‡ä»¶å¤¹å†…å®¹ï¼ˆå…¨é‡é‡å»ºï¼‰
// maxDepth æ§åˆ¶é€’å½’æ·±åº¦ï¼Œ0 è¡¨ç¤ºåªå¤„ç†å½“å‰ç›®å½•ï¼Œ-1 è¡¨ç¤ºæ— é™æ·±åº¦
func (e *ExternalIndexer) IndexFolderContent(folderPath, sourceDocID, blockID string, maxDepth int) (*FolderIndexResult, error) {
	fmt.Printf("\nğŸ“ [RAG] IndexFolderContent called: folder=%s, docID=%s, blockID=%s\n", folderPath, sourceDocID, blockID)

	// 1. è®¾ç½®é»˜è®¤æ·±åº¦
	if maxDepth <= 0 {
		maxDepth = 10 // é»˜è®¤æœ€å¤§ 10 å±‚
	}

	// 2. ç”ŸæˆåŸºç¡€ ID å¹¶åˆ é™¤æ—§æ•°æ®
	baseID := fmt.Sprintf("%s_%s_folder", sourceDocID, blockID)
	if err := e.store.DeleteBlocksByPrefix(baseID); err != nil {
		fmt.Printf("âš ï¸ [RAG] Failed to delete old folder chunks for %s: %v\n", baseID, err)
	}

	// 3. æ”¶é›†æ–‡ä»¶å¤¹ä¸­æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶
	var files []string
	err := e.walkFolder(folderPath, 0, maxDepth, &files)
	if err != nil {
		fmt.Printf("âŒ [RAG] Failed to walk folder: %v\n", err)
		return nil, fmt.Errorf("failed to walk folder: %w", err)
	}

	fmt.Printf("ğŸ“ [RAG] Found %d supported files in folder\n", len(files))
	if debugChunks {
		for i, f := range files {
			fmt.Printf("   [%d] %s\n", i, f)
		}
	}

	if len(files) == 0 {
		fmt.Printf("ğŸ“ [RAG] No supported files found in folder, returning empty result\n")
		return &FolderIndexResult{
			TotalFiles:   0,
			SuccessCount: 0,
			FailedCount:  0,
			FailedFiles:  nil,
		}, nil
	}

	// 4. ç´¢å¼•æ¯ä¸ªæ–‡ä»¶
	result := &FolderIndexResult{
		TotalFiles:  len(files),
		FailedFiles: make([]string, 0),
	}

	folderName := filepath.Base(folderPath)

	for fileIndex, filePath := range files {
		// æå–æ–‡æœ¬å†…å®¹
		textContent, err := fileextract.ExtractText(filePath)
		if err != nil {
			result.FailedCount++
			result.FailedFiles = append(result.FailedFiles, filepath.Base(filePath))
			fmt.Printf("âš ï¸ [RAG] Failed to extract text from %s: %v\n", filePath, err)
			continue
		}

		if textContent == "" {
			result.FailedCount++
			result.FailedFiles = append(result.FailedFiles, filepath.Base(filePath))
			continue
		}

		// æ„å»ºä¸Šä¸‹æ–‡ï¼ˆæ–‡ä»¶å¤¹å/æ–‡ä»¶åï¼‰
		fileName := filepath.Base(filePath)
		headingContext := fmt.Sprintf("%s/%s", folderName, fileName)

		// ç”Ÿæˆæ–‡ä»¶çº§åˆ«çš„ ID
		fileID := fmt.Sprintf("%s_%d", baseID, fileIndex)

		// å¯¹å†…å®¹è¿›è¡Œåˆ†å—
		chunks := ChunkTextContent(textContent, headingContext, fileID, e.indexer.chunkConfig)

		if len(chunks) == 0 {
			chunks = []ExtractedBlock{{
				ID:             fileID,
				Type:           "folder",
				Content:        textContent,
				HeadingContext: headingContext,
			}}
		}

		// ä¸ºæ¯ä¸ª chunk ç”Ÿæˆ embedding å¹¶å­˜å‚¨
		fileSuccess := false
		for _, chunk := range chunks {
			if chunk.Content == "" {
				continue
			}

			embedding, err := e.embedder.Embed(chunk.Content)
			if err != nil {
				fmt.Printf("âš ï¸ [RAG] Failed to embed folder chunk %s: %v\n", chunk.ID, err)
				continue
			}

			contentHash := HashContent(chunk.Content)
			if err := e.store.Upsert(&BlockVector{
				ID:             chunk.ID,
				SourceBlockID:  blockID,
				DocID:          sourceDocID,
				Content:        chunk.Content,
				ContentHash:    contentHash,
				BlockType:      "folder",
				HeadingContext: chunk.HeadingContext,
				FilePath:       filePath,
				Embedding:      embedding,
			}); err != nil {
				fmt.Printf("âš ï¸ [RAG] Failed to upsert folder chunk %s: %v\n", chunk.ID, err)
			} else {
				fileSuccess = true
			}
		}

		if fileSuccess {
			result.SuccessCount++
		} else {
			result.FailedCount++
			result.FailedFiles = append(result.FailedFiles, fileName)
		}
	}

	// 5. ä¿å­˜æ–‡ä»¶å¤¹çº§åˆ«å…ƒæ•°æ®
	if err := e.store.SaveExternalContent(&ExternalBlockContent{
		ID:          fmt.Sprintf("%s_%s", sourceDocID, blockID),
		DocID:       sourceDocID,
		BlockID:     blockID,
		BlockType:   "folder",
		FilePath:    folderPath,
		Title:       folderName,
		RawContent:  fmt.Sprintf("Folder: %s\nTotal files: %d\nIndexed: %d", folderPath, result.TotalFiles, result.SuccessCount),
		ExtractedAt: time.Now().Unix(),
	}); err != nil {
		fmt.Printf("âš ï¸ [RAG] Failed to save folder metadata for %s: %v\n", baseID, err)
	}

	fmt.Printf("âœ… [RAG] Folder indexing complete: %d/%d files indexed\n", result.SuccessCount, result.TotalFiles)
	return result, nil
}

// walkFolder é€’å½’éå†æ–‡ä»¶å¤¹ï¼Œæ”¶é›†æ”¯æŒçš„æ–‡ä»¶
func (e *ExternalIndexer) walkFolder(dir string, currentDepth, maxDepth int, files *[]string) error {
	if currentDepth > maxDepth {
		return nil
	}

	entries, err := os.ReadDir(dir)
	if err != nil {
		return err
	}

	for _, entry := range entries {
		fullPath := filepath.Join(dir, entry.Name())

		if entry.IsDir() {
			// è·³è¿‡éšè—ç›®å½•å’Œå¸¸è§çš„æ— å…³ç›®å½•
			name := entry.Name()
			if strings.HasPrefix(name, ".") || name == "node_modules" || name == "vendor" || name == "__pycache__" {
				continue
			}
			// é€’å½’å¤„ç†å­ç›®å½•
			if err := e.walkFolder(fullPath, currentDepth+1, maxDepth, files); err != nil {
				fmt.Printf("âš ï¸ [RAG] Failed to walk subdir %s: %v\n", fullPath, err)
			}
		} else {
			// æ£€æŸ¥æ˜¯å¦æ˜¯æ”¯æŒçš„æ–‡ä»¶ç±»å‹
			ext := strings.ToLower(filepath.Ext(entry.Name()))
			if supportedExtensions[ext] {
				*files = append(*files, fullPath)
			}
		}
	}

	return nil
}

// ReindexAll é‡æ–°ç´¢å¼•æ‰€æœ‰ bookmark å’Œ file å—
// éå†æ‰€æœ‰æ–‡æ¡£ï¼Œæå– bookmark/file å—ä¿¡æ¯ï¼Œç„¶åé‡æ–°æŠ“å–å’Œç´¢å¼•

func (e *ExternalIndexer) ReindexAll() (int, error) {
	// è·å–æ‰€æœ‰æ–‡æ¡£
	index, err := e.docRepo.GetAll()
	if err != nil {
		return 0, fmt.Errorf("failed to get documents: %w", err)
	}

	totalCount := 0
	for _, doc := range index.Documents {
		// åŠ è½½æ–‡æ¡£å†…å®¹
		content, err := e.docStorage.Load(doc.ID)
		if err != nil {
			fmt.Printf("âš ï¸ [RAG] Failed to load document %s: %v\n", doc.ID, err)
			continue
		}

		// æå–å¤–éƒ¨å—ä¿¡æ¯
		externalIDs := ExtractExternalBlockIDs([]byte(content))

		// é‡æ–°ç´¢å¼• bookmark å—
		for _, bookmark := range externalIDs.BookmarkBlocks {
			if bookmark.URL == "" {
				continue
			}
			if err := e.IndexBookmarkContent(bookmark.URL, doc.ID, bookmark.BlockID); err != nil {
				fmt.Printf("âš ï¸ [RAG] Failed to reindex bookmark %s: %v\n", bookmark.BlockID, err)
			} else {
				totalCount++
				fmt.Printf("âœ… [RAG] Reindexed bookmark: %s\n", bookmark.URL)
			}
		}

		// é‡æ–°ç´¢å¼• file å—
		for _, file := range externalIDs.FileBlocks {
			if file.FilePath == "" {
				continue
			}
			if err := e.IndexFileContent(file.FilePath, doc.ID, file.BlockID); err != nil {
				fmt.Printf("âš ï¸ [RAG] Failed to reindex file %s: %v\n", file.BlockID, err)
			} else {
				totalCount++
				fmt.Printf("âœ… [RAG] Reindexed file: %s\n", file.FilePath)
			}
		}

		// é‡æ–°ç´¢å¼• folder å—
		for _, folder := range externalIDs.FolderBlocks {
			if folder.FolderPath == "" {
				continue
			}
			if _, err := e.IndexFolderContent(folder.FolderPath, doc.ID, folder.BlockID, 0); err != nil {
				fmt.Printf("âš ï¸ [RAG] Failed to reindex folder %s: %v\n", folder.BlockID, err)
			} else {
				totalCount++
				fmt.Printf("âœ… [RAG] Reindexed folder: %s\n", folder.FolderPath)
			}
		}
	}

	return totalCount, nil
}

// ReindexAllWithProgress é‡æ–°ç´¢å¼•æ‰€æœ‰ bookmark å’Œ file å—ï¼ˆå¸¦è¿›åº¦å›è°ƒï¼‰
func (e *ExternalIndexer) ReindexAllWithProgress(onProgress func(current, total int)) (int, error) {
	// è·å–æ‰€æœ‰æ–‡æ¡£å¹¶è®¡ç®—å¤–éƒ¨å—æ€»æ•°
	index, err := e.docRepo.GetAll()
	if err != nil {
		return 0, fmt.Errorf("failed to get documents: %w", err)
	}

	// å…ˆç»Ÿè®¡æ€»æ•°
	var allExternalBlocks []struct {
		docID    string
		bookmark *BookmarkBlockInfo
		file     *FileBlockInfo
		folder   *FolderBlockInfo
	}

	for _, doc := range index.Documents {
		content, err := e.docStorage.Load(doc.ID)
		if err != nil {
			continue
		}
		externalIDs := ExtractExternalBlockIDs([]byte(content))
		for i := range externalIDs.BookmarkBlocks {
			if externalIDs.BookmarkBlocks[i].URL != "" {
				allExternalBlocks = append(allExternalBlocks, struct {
					docID    string
					bookmark *BookmarkBlockInfo
					file     *FileBlockInfo
					folder   *FolderBlockInfo
				}{docID: doc.ID, bookmark: &externalIDs.BookmarkBlocks[i]})
			}
		}
		for i := range externalIDs.FileBlocks {
			if externalIDs.FileBlocks[i].FilePath != "" {
				allExternalBlocks = append(allExternalBlocks, struct {
					docID    string
					bookmark *BookmarkBlockInfo
					file     *FileBlockInfo
					folder   *FolderBlockInfo
				}{docID: doc.ID, file: &externalIDs.FileBlocks[i]})
			}
		}
		for i := range externalIDs.FolderBlocks {
			if externalIDs.FolderBlocks[i].FolderPath != "" {
				allExternalBlocks = append(allExternalBlocks, struct {
					docID    string
					bookmark *BookmarkBlockInfo
					file     *FileBlockInfo
					folder   *FolderBlockInfo
				}{docID: doc.ID, folder: &externalIDs.FolderBlocks[i]})
			}
		}
	}

	total := len(allExternalBlocks)
	if total == 0 {
		return 0, nil
	}

	successCount := 0
	for i, block := range allExternalBlocks {
		// å‘é€è¿›åº¦
		if onProgress != nil {
			onProgress(i+1, total)
		}

		if block.bookmark != nil {
			if err := e.IndexBookmarkContent(block.bookmark.URL, block.docID, block.bookmark.BlockID); err != nil {
				fmt.Printf("âš ï¸ [RAG] Failed to reindex bookmark %s: %v\n", block.bookmark.BlockID, err)
			} else {
				successCount++
				fmt.Printf("âœ… [RAG] Reindexed bookmark: %s\n", block.bookmark.URL)
			}
		} else if block.file != nil {
			if err := e.IndexFileContent(block.file.FilePath, block.docID, block.file.BlockID); err != nil {
				fmt.Printf("âš ï¸ [RAG] Failed to reindex file %s: %v\n", block.file.BlockID, err)
			} else {
				successCount++
				fmt.Printf("âœ… [RAG] Reindexed file: %s\n", block.file.FilePath)
			}
		} else if block.folder != nil {
			if _, err := e.IndexFolderContent(block.folder.FolderPath, block.docID, block.folder.BlockID, 0); err != nil {
				fmt.Printf("âš ï¸ [RAG] Failed to reindex folder %s: %v\n", block.folder.BlockID, err)
			} else {
				successCount++
				fmt.Printf("âœ… [RAG] Reindexed folder: %s\n", block.folder.FolderPath)
			}
		}
	}

	return successCount, nil
}
