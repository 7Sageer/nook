package rag

import (
	"fmt"
	"os"
	"strings"

	"notion-lite/internal/document"
)

// debugChunks æ˜¯å¦è¾“å‡º chunk è°ƒè¯•ä¿¡æ¯ï¼ˆé€šè¿‡ç¯å¢ƒå˜é‡ DEBUG_RAG_CHUNKS=1 å¯ç”¨ï¼‰
var debugChunks = os.Getenv("DEBUG_RAG_CHUNKS") == "1"

// truncateContent æˆªæ–­å†…å®¹ç”¨äºæ˜¾ç¤º
func truncateContent(s string, maxLen int) string {
	s = strings.ReplaceAll(s, "\n", "â†µ ")
	if len(s) > maxLen {
		return s[:maxLen] + "..."
	}
	return s
}

// Indexer æ–‡æ¡£ç´¢å¼•å™¨
type Indexer struct {
	store       *VectorStore
	embedder    EmbeddingClient
	docRepo     *document.Repository
	docStorage  *document.Storage
	chunkConfig ChunkConfig
}

// NewIndexer åˆ›å»ºç´¢å¼•å™¨
func NewIndexer(store *VectorStore, embedder EmbeddingClient, docRepo *document.Repository, docStorage *document.Storage) *Indexer {
	return &Indexer{
		store:       store,
		embedder:    embedder,
		docRepo:     docRepo,
		docStorage:  docStorage,
		chunkConfig: DefaultChunkConfig,
	}
}

// NewIndexerWithConfig åˆ›å»ºå¸¦é…ç½®çš„ç´¢å¼•å™¨
func NewIndexerWithConfig(store *VectorStore, embedder EmbeddingClient, docRepo *document.Repository, docStorage *document.Storage, config ChunkConfig) *Indexer {
	return &Indexer{
		store:       store,
		embedder:    embedder,
		docRepo:     docRepo,
		docStorage:  docStorage,
		chunkConfig: config,
	}
}

// SetChunkConfig æ›´æ–°åˆ†å—é…ç½®
func (idx *Indexer) SetChunkConfig(config ChunkConfig) {
	idx.chunkConfig = config
}

// IndexDocument ç´¢å¼•å•ä¸ªæ–‡æ¡£ï¼ˆå¢é‡æ›´æ–°ï¼‰
func (idx *Indexer) IndexDocument(docID string) error {
	// 1. åŠ è½½æ–‡æ¡£å†…å®¹
	content, err := idx.docStorage.Load(docID)
	if err != nil {
		return fmt.Errorf("failed to load document: %w", err)
	}

	// 2. è·å–ç°æœ‰å—çš„å“ˆå¸Œ
	existingHashes, err := idx.store.GetBlockHashes(docID)
	if err != nil {
		existingHashes = make(map[string]string)
	}

	// 3. ä½¿ç”¨é…ç½®æå–æ–°å—å¹¶è®¡ç®—å“ˆå¸Œ
	blocks := ExtractBlocksWithConfig([]byte(content), idx.chunkConfig)
	newBlockIDs := make(map[string]bool)

	// è°ƒè¯•è¾“å‡ºï¼šæ˜¾ç¤ºåˆ†å—è¯¦æƒ…
	if debugChunks {
		fmt.Printf("\nğŸ“„ [RAG] Indexing document: %s\n", docID)
		fmt.Printf("   Total chunks: %d\n", len(blocks))
		fmt.Println("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
		for i, block := range blocks {
			fmt.Printf("   [%d] Type: %-25s | Heading: %s\n",
				i, block.Type, truncateContent(block.HeadingContext, 30))
			fmt.Printf("       Content (%4d chars): %s\n",
				len(block.Content), truncateContent(block.Content, 80))
		}
		fmt.Println("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
	}

	for _, block := range blocks {
		if block.Content == "" {
			continue
		}
		newBlockIDs[block.ID] = true
		newHash := HashContent(block.Content + block.HeadingContext)

		// æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
		if oldHash, exists := existingHashes[block.ID]; exists && oldHash == newHash {
			// å†…å®¹æ²¡å˜ï¼Œè·³è¿‡
			continue
		}

		// éœ€è¦æ›´æ–°ï¼šç”Ÿæˆæ–°çš„ Embedding
		embedding, err := idx.embedder.Embed(block.Content)
		if err != nil {
			continue
		}
		// è‹¥ block æœ¬èº«æ˜¯èšåˆ/åˆå¹¶å—ï¼Œä½¿ç”¨å…¶ SourceBlockIDï¼›å¦åˆ™ä½¿ç”¨ block.ID
		sourceBlockID := block.SourceBlockID
		if sourceBlockID == "" {
			sourceBlockID = block.ID
		}
		idx.store.Upsert(&BlockVector{
			ID:             block.ID,
			SourceBlockID:  sourceBlockID,
			DocID:          docID,
			Content:        block.Content,
			ContentHash:    newHash,
			BlockType:      block.Type,
			HeadingContext: block.HeadingContext,
			Embedding:      embedding,
		})
	}

	// 4. åˆ é™¤å·²ä¸å­˜åœ¨çš„å—
	var toDelete []string
	for id := range existingHashes {
		// å¸¸è§„å—ï¼šå¦‚æœåœ¨æ–°çš„å—åˆ—è¡¨ä¸­ä¸å­˜åœ¨ï¼Œä¸”ä¸æ˜¯ bookmarkï¼Œåˆ™åˆ é™¤
		if !newBlockIDs[id] && !strings.Contains(id, "_bookmark") {
			toDelete = append(toDelete, id)
		}
	}
	if len(toDelete) > 0 {
		idx.store.DeleteBlocks(toDelete)
	}

	// 5. æ¸…ç†å­¤å„¿ bookmarkï¼ˆå³åœ¨ç¼–è¾‘å™¨ä¸­å·²åˆ é™¤çš„ bookmarkï¼‰
	currentBookmarkBlockIDs := ExtractBookmarkBlockIDs([]byte(content))
	if err := idx.store.DeleteOrphanBookmarks(docID, currentBookmarkBlockIDs); err != nil {
		fmt.Printf("âš ï¸ [RAG] Failed to delete orphan bookmarks for doc %s: %v\n", docID, err)
	}

	return nil
}

// ForceReindexDocument å¼ºåˆ¶é‡å»ºå•ä¸ªæ–‡æ¡£ç´¢å¼•ï¼ˆåˆ é™¤æ‰€æœ‰æ—§å—åé‡æ–°ç´¢å¼•ï¼‰
func (idx *Indexer) ForceReindexDocument(docID string) error {
	// 1. åŠ è½½æ–‡æ¡£å†…å®¹
	content, err := idx.docStorage.Load(docID)
	if err != nil {
		return fmt.Errorf("failed to load document: %w", err)
	}

	// 2. æ¸…ç†æ—§ç´¢å¼•
	// åˆ é™¤è¯¥æ–‡æ¡£çš„æ‰€æœ‰é bookmark å—
	idx.store.DeleteNonBookmarkByDocID(docID)

	// æ¸…ç†å­¤å„¿ bookmarkï¼ˆä¿ç•™å½“å‰å­˜åœ¨çš„ï¼‰
	currentBookmarkBlockIDs := ExtractBookmarkBlockIDs([]byte(content))
	if err := idx.store.DeleteOrphanBookmarks(docID, currentBookmarkBlockIDs); err != nil {
		fmt.Printf("âš ï¸ [RAG] Failed to delete orphan bookmarks for doc %s: %v\n", docID, err)
	}

	// 3. ä½¿ç”¨æ–°é…ç½®æå–å—
	blocks := ExtractBlocksWithConfig([]byte(content), idx.chunkConfig)

	// è°ƒè¯•è¾“å‡º
	if debugChunks {
		fmt.Printf("\nğŸ“„ [RAG] Force reindexing document: %s\n", docID)
		fmt.Printf("   Total chunks: %d\n", len(blocks))
		fmt.Println("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
		for i, block := range blocks {
			fmt.Printf("   [%d] Type: %-25s | Heading: %s\n",
				i, block.Type, truncateContent(block.HeadingContext, 30))
			fmt.Printf("       Content (%4d chars): %s\n",
				len(block.Content), truncateContent(block.Content, 80))
		}
		fmt.Println("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
	}

	// 4. ä¸ºæ¯ä¸ªå—ç”Ÿæˆ embedding å¹¶å­˜å‚¨
	for _, block := range blocks {
		if block.Content == "" {
			continue
		}

		embedding, err := idx.embedder.Embed(block.Content)
		if err != nil {
			continue
		}

		// è‹¥ block æœ¬èº«æ˜¯èšåˆ/åˆå¹¶å—ï¼Œä½¿ç”¨å…¶ SourceBlockIDï¼›å¦åˆ™ä½¿ç”¨ block.ID
		sourceBlockID := block.SourceBlockID
		if sourceBlockID == "" {
			sourceBlockID = block.ID
		}

		newHash := HashContent(block.Content + block.HeadingContext)
		idx.store.Upsert(&BlockVector{
			ID:             block.ID,
			SourceBlockID:  sourceBlockID,
			DocID:          docID,
			Content:        block.Content,
			ContentHash:    newHash,
			BlockType:      block.Type,
			HeadingContext: block.HeadingContext,
			Embedding:      embedding,
		})
	}

	return nil
}

// ReindexAll é‡å»ºæ‰€æœ‰æ–‡æ¡£ç´¢å¼•ï¼ˆå¼ºåˆ¶æ¨¡å¼ï¼Œæ¸…é™¤æ—§æ•°æ®ï¼Œæ¸…ç†å­¤å„¿å—ï¼‰
func (idx *Indexer) ReindexAll() (int, error) {
	index, err := idx.docRepo.GetAll()
	if err != nil {
		return 0, fmt.Errorf("failed to get documents: %w", err)
	}

	// æ„å»ºç°æœ‰æ–‡æ¡£ ID é›†åˆ
	existingDocIDs := make(map[string]bool)
	for _, doc := range index.Documents {
		existingDocIDs[doc.ID] = true
	}

	// æ¸…ç†å·²åˆ é™¤æ–‡æ¡£çš„å­¤å„¿å—
	indexedDocIDs, err := idx.store.GetAllDocIDs()
	if err == nil {
		for _, docID := range indexedDocIDs {
			if !existingDocIDs[docID] {
				if debugChunks {
					fmt.Printf("ğŸ—‘ï¸ [RAG] Cleaning orphan blocks for deleted document: %s\n", docID)
				}
				idx.store.DeleteByDocID(docID)
			}
		}
	}

	// é‡å»ºç´¢å¼•
	count := 0
	for _, doc := range index.Documents {
		if err := idx.ForceReindexDocument(doc.ID); err != nil {
			continue // è·³è¿‡å¤±è´¥çš„æ–‡æ¡£
		}
		count++
	}
	return count, nil
}
