package rag

import (
	"context"
	"encoding/json"
	"fmt"
	"notion-lite/internal/document"
	"notion-lite/internal/utils"
	"os"
	"strings"
)

// Service RAG æœåŠ¡ç»Ÿä¸€å…¥å£
type Service struct {
	ctx             context.Context
	paths           *utils.PathBuilder
	store           *VectorStore
	indexer         *Indexer
	searcher        *Searcher
	externalIndexer *ExternalIndexer
	embedder        EmbeddingClient
	docRepo         *document.Repository
	docStorage      *document.Storage
}

// NewService åˆ›å»º RAG æœåŠ¡
func NewService(paths *utils.PathBuilder, docRepo *document.Repository, docStorage *document.Storage) *Service {
	return &Service{
		paths:      paths,
		docRepo:    docRepo,
		docStorage: docStorage,
	}
}

// init åˆå§‹åŒ–å†…éƒ¨ç»„ä»¶ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
func (s *Service) init() error {
	if s.embedder != nil {
		return nil // å·²åˆå§‹åŒ–
	}

	// åŠ è½½é…ç½®
	config, err := LoadConfig(s.paths)
	if err != nil {
		return err
	}

	// åˆ›å»º Embedding å®¢æˆ·ç«¯
	embedder, err := NewEmbeddingClient(config)
	if err != nil {
		return err
	}
	s.embedder = embedder

	// åˆ›å»ºå‘é‡å­˜å‚¨
	dbPath := s.paths.RAGDatabase()
	store, err := NewVectorStore(dbPath, embedder.Dimension())
	if err != nil {
		return err
	}
	s.store = store

	// åˆ›å»ºç´¢å¼•å™¨å’Œæœç´¢å™¨
	s.indexer = NewIndexer(store, embedder, s.docRepo, s.docStorage, s.paths)
	s.searcher = NewSearcher(store, embedder, s.docRepo)
	s.externalIndexer = NewExternalIndexer(store, embedder, s.docRepo, s.docStorage, s.indexer, s.paths)

	return nil
}

// Warmup é¢„çƒ­åˆå§‹åŒ–ï¼ˆåªåŠ è½½ç»„ä»¶ï¼Œä¸åšå®é™…æœç´¢ï¼‰
// ç”¨äºåœ¨åº”ç”¨ç©ºé—²æ—¶æå‰åˆå§‹åŒ–ï¼Œé¿å…é¦–æ¬¡ä½¿ç”¨æ—¶çš„å†·å¯åŠ¨å»¶è¿Ÿ
func (s *Service) Warmup() error {
	return s.init()
}

// IndexDocument ç´¢å¼•å•ä¸ªæ–‡æ¡£
func (s *Service) IndexDocument(docID string) error {
	if err := s.init(); err != nil {
		return err
	}
	return s.indexer.IndexDocument(docID)
}

// SearchDocuments æ–‡æ¡£çº§è¯­ä¹‰æœç´¢ï¼ˆèšåˆ chunksï¼‰
func (s *Service) SearchDocuments(query string, limit int, filter *SearchFilter) ([]DocumentSearchResult, error) {
	if err := s.init(); err != nil {
		return nil, err
	}
	return s.searcher.SearchDocuments(query, limit, filter)
}

// SearchChunks å—çº§è¯­ä¹‰æœç´¢
func (s *Service) SearchChunks(query string, limit int, filter *SearchFilter) ([]ChunkMatch, error) {
	if err := s.init(); err != nil {
		return nil, err
	}
	return s.searcher.SearchChunks(query, limit, filter)
}

// ReindexAll é‡å»ºæ‰€æœ‰æ–‡æ¡£ç´¢å¼•
func (s *Service) ReindexAll() (int, error) {
	if err := s.init(); err != nil {
		return 0, err
	}
	return s.indexer.ReindexAll()
}

// SetContext è®¾ç½® Wails ä¸Šä¸‹æ–‡ï¼ˆç”¨äºå‘é€äº‹ä»¶ï¼‰
func (s *Service) SetContext(ctx context.Context) {
	s.ctx = ctx
}

// ReindexAllWithProgress é‡å»ºæ‰€æœ‰æ–‡æ¡£ç´¢å¼•ï¼ˆå¸¦è¿›åº¦å›è°ƒï¼‰
func (s *Service) ReindexAllWithProgress(onProgress func(current, total int)) (int, error) {
	if err := s.init(); err != nil {
		return 0, err
	}
	return s.indexer.ReindexAllWithCallback(onProgress)
}

// DeleteDocument åˆ é™¤æ–‡æ¡£çš„æ‰€æœ‰å‘é‡ç´¢å¼•
func (s *Service) DeleteDocument(docID string) error {
	if err := s.init(); err != nil {
		return err
	}
	return s.store.DeleteByDocID(docID)
}

// GetIndexedCount è·å–å·²ç´¢å¼•çš„æ–‡æ¡£æ•°é‡
func (s *Service) GetIndexedCount() (int, error) {
	if err := s.init(); err != nil {
		return 0, nil // åˆå§‹åŒ–å¤±è´¥ï¼Œè¿”å› 0
	}
	return s.store.GetIndexedDocCount()
}

// GetIndexedStats è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯ (æ–‡æ¡£æ•°, ä¹¦ç­¾æ•°, åµŒå…¥æ–‡ä»¶æ•°, æ–‡ä»¶å¤¹æ•°)
func (s *Service) GetIndexedStats() (int, int, int, int, error) {
	if err := s.init(); err != nil {
		return 0, 0, 0, 0, nil // åˆå§‹åŒ–å¤±è´¥ï¼Œè¿”å› 0
	}
	return s.store.GetIndexedStats()
}

// Reinitialize é‡æ–°åˆå§‹åŒ–ï¼ˆé…ç½®å˜æ›´åè°ƒç”¨ï¼‰
// å¦‚æœæ–°æ¨¡å‹çš„ç»´åº¦ä¸æ—§æ¨¡å‹ä¸åŒï¼Œä¼šè‡ªåŠ¨åˆ é™¤å‘é‡æ•°æ®åº“
func (s *Service) Reinitialize() error {
	// è·å–æ—§çš„ç»´åº¦
	oldDimension := 0
	if s.embedder != nil {
		oldDimension = s.embedder.Dimension()
	}

	// å…³é—­æ—§çš„å­˜å‚¨
	if s.store != nil {
		if err := s.store.Close(); err != nil {
			fmt.Printf("âš ï¸ [RAG] Failed to close store: %v\n", err)
		}
	}

	// é‡ç½®æ‰€æœ‰ç»„ä»¶
	s.store = nil
	s.indexer = nil
	s.searcher = nil
	s.embedder = nil

	// åŠ è½½æ–°é…ç½®ï¼Œæ£€æŸ¥ç»´åº¦æ˜¯å¦å˜åŒ–
	config, err := LoadConfig(s.paths)
	if err != nil {
		return err
	}

	newEmbedder, err := NewEmbeddingClient(config)
	if err != nil {
		return err
	}
	newDimension := newEmbedder.Dimension()

	// æ£€æŸ¥ç»´åº¦æ˜¯å¦å˜åŒ–
	dimensionChanged := oldDimension > 0 && oldDimension != newDimension

	// å¦‚æœç»´åº¦å˜åŒ–ï¼Œåˆ é™¤æ—§çš„å‘é‡æ•°æ®åº“
	if dimensionChanged {
		dbPath := s.paths.RAGDatabase()
		fmt.Printf("ğŸ”„ [RAG] Dimension changed (%d â†’ %d), removing old database...\n", oldDimension, newDimension)
		if err := os.Remove(dbPath); err != nil && !os.IsNotExist(err) {
			fmt.Printf("âš ï¸ [RAG] Failed to remove old database: %v\n", err)
		}
	}

	// é‡æ–°åˆå§‹åŒ–
	s.embedder = newEmbedder

	dbPath := s.paths.RAGDatabase()
	store, err := NewVectorStore(dbPath, newDimension)
	if err != nil {
		return err
	}
	s.store = store

	s.indexer = NewIndexer(store, s.embedder, s.docRepo, s.docStorage, s.paths)
	s.searcher = NewSearcher(store, s.embedder, s.docRepo)
	s.externalIndexer = NewExternalIndexer(store, s.embedder, s.docRepo, s.docStorage, s.indexer, s.paths)

	// å¦‚æœç»´åº¦å˜åŒ–ï¼Œè‡ªåŠ¨è§¦å‘å…¨é‡é‡å»ºç´¢å¼•ï¼ˆåŒ…æ‹¬ bookmark å’Œ file å—ï¼‰
	if dimensionChanged {
		go func() {
			fmt.Println("ğŸ”„ [RAG] Starting automatic reindex due to dimension change...")
			if count, err := s.ReindexAll(); err != nil {
				fmt.Printf("âš ï¸ [RAG] ReindexAll failed: %v\n", err)
			} else {
				fmt.Printf("âœ… [RAG] Reindexed %d documents\n", count)
			}
			if extCount, err := s.ReindexExternalContent(); err != nil {
				fmt.Printf("âš ï¸ [RAG] ReindexExternalContent failed: %v\n", err)
			} else {
				fmt.Printf("âœ… [RAG] Reindexed %d external blocks (bookmarks + files)\n", extCount)
			}
		}()
	}

	return nil
}

// ReindexExternalContent é‡æ–°ç´¢å¼•æ‰€æœ‰ bookmark å’Œ file å—
func (s *Service) ReindexExternalContent() (int, error) {
	if err := s.init(); err != nil {
		return 0, err
	}
	return s.externalIndexer.ReindexAll()
}

// ReindexExternalContentWithProgress é‡æ–°ç´¢å¼•æ‰€æœ‰ bookmark å’Œ file å—ï¼ˆå¸¦è¿›åº¦å›è°ƒï¼‰
func (s *Service) ReindexExternalContentWithProgress(onProgress func(current, total int)) (int, error) {
	if err := s.init(); err != nil {
		return 0, err
	}
	return s.externalIndexer.ReindexAllWithProgress(onProgress)
}

// IndexBookmarkContent ç´¢å¼•ä¹¦ç­¾ç½‘é¡µå†…å®¹
func (s *Service) IndexBookmarkContent(url, sourceDocID, blockID string) error {
	if err := s.init(); err != nil {
		return err
	}
	return s.externalIndexer.IndexBookmarkContent(url, sourceDocID, blockID)
}

// IndexFileContent ç´¢å¼•æ–‡ä»¶å†…å®¹
func (s *Service) IndexFileContent(filePath, sourceDocID, blockID, fileName string) error {
	if err := s.init(); err != nil {
		return err
	}
	return s.externalIndexer.IndexFileContent(filePath, sourceDocID, blockID, fileName)
}

// GetExternalBlockContent è·å–å¤–éƒ¨å—çš„å®Œæ•´æå–å†…å®¹
func (s *Service) GetExternalBlockContent(docID, blockID string) (*ExternalBlockContent, error) {
	if err := s.init(); err != nil {
		return nil, err
	}
	return s.store.GetExternalContent(docID, blockID)
}

// IndexFolderContent ç´¢å¼•æ–‡ä»¶å¤¹å†…å®¹
func (s *Service) IndexFolderContent(folderPath, sourceDocID, blockID string) (*FolderIndexResult, error) {
	if err := s.init(); err != nil {
		return nil, err
	}
	return s.externalIndexer.IndexFolderContent(folderPath, sourceDocID, blockID, 10)
}

// SearchSimilarDocuments æœç´¢ä¸æŒ‡å®šæ–‡æ¡£ç›¸ä¼¼çš„æ–‡æ¡£ï¼ˆç”¨äº tag æ¨èï¼‰
func (s *Service) SearchSimilarDocuments(docID string, limit int) ([]SimilarDocResult, error) {
	if err := s.init(); err != nil {
		return nil, err
	}

	// è·å–æ–‡æ¡£å†…å®¹
	content, err := s.docStorage.Load(docID)
	if err != nil {
		return nil, err
	}

	// æå–çº¯æ–‡æœ¬ä½œä¸ºæŸ¥è¯¢ï¼ˆæœ€å¤š 500 å­—ï¼‰
	query := extractPlainText([]byte(content), 500)
	if query == "" {
		return nil, nil
	}

	// æœç´¢ç›¸ä¼¼æ–‡æ¡£ï¼ˆæ’é™¤å½“å‰æ–‡æ¡£ï¼‰
	filter := &SearchFilter{ExcludeDocID: docID}
	results, err := s.searcher.SearchDocuments(query, limit, filter)
	if err != nil {
		return nil, err
	}

	// è½¬æ¢ç»“æœ
	similar := make([]SimilarDocResult, len(results))
	for i, r := range results {
		similar[i] = SimilarDocResult{DocID: r.DocID}
	}
	return similar, nil
}

// SimilarDocResult ç›¸ä¼¼æ–‡æ¡£ç»“æœ
type SimilarDocResult struct {
	DocID string `json:"docId"`
}

// extractPlainText ä»æ–‡æ¡£å†…å®¹æå–çº¯æ–‡æœ¬ï¼ˆç”¨äºè¯­ä¹‰æœç´¢æŸ¥è¯¢ï¼‰
func extractPlainText(content []byte, maxChars int) string {
	var blocks []interface{}
	if err := json.Unmarshal(content, &blocks); err != nil {
		return ""
	}

	var texts []string
	extractTextsFromBlocks(blocks, &texts)

	result := strings.Join(texts, " ")
	if maxChars > 0 && len(result) > maxChars {
		result = result[:maxChars]
	}
	return strings.TrimSpace(result)
}

// extractTextsFromBlocks é€’å½’æå–æ‰€æœ‰å—çš„æ–‡æœ¬
func extractTextsFromBlocks(blocks []interface{}, texts *[]string) {
	for _, block := range blocks {
		blockMap, ok := block.(map[string]interface{})
		if !ok {
			continue
		}

		// æå–å½“å‰å—çš„æ–‡æœ¬å†…å®¹
		if content, ok := blockMap["content"].([]interface{}); ok {
			text := extractTextFromContentItems(content)
			if text != "" {
				*texts = append(*texts, text)
			}
		}

		// é€’å½’å¤„ç†å­å—
		if children, ok := blockMap["children"].([]interface{}); ok {
			extractTextsFromBlocks(children, texts)
		}
	}
}

// extractTextFromContentItems ä» content æ•°ç»„æå–æ–‡æœ¬
func extractTextFromContentItems(content []interface{}) string {
	var parts []string
	for _, item := range content {
		itemMap, ok := item.(map[string]interface{})
		if !ok {
			continue
		}
		if itemMap["type"] == "text" {
			if text, ok := itemMap["text"].(string); ok {
				parts = append(parts, text)
			}
		}
		if itemMap["type"] == "link" {
			if linkContent, ok := itemMap["content"].([]interface{}); ok {
				parts = append(parts, extractTextFromContentItems(linkContent))
			}
		}
	}
	return strings.Join(parts, "")
}
