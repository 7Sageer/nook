package rag

import (
	"context"
	"fmt"
	"os"
	"path/filepath"
	"strings"

	"notion-lite/internal/document"
	"notion-lite/internal/fileextract"
	"notion-lite/internal/opengraph"
)

// Service RAG æœåŠ¡ç»Ÿä¸€å…¥å£
type Service struct {
	ctx        context.Context
	dataPath   string
	store      *VectorStore
	indexer    *Indexer
	searcher   *Searcher
	embedder   EmbeddingClient
	docRepo    *document.Repository
	docStorage *document.Storage
}

// NewService åˆ›å»º RAG æœåŠ¡
func NewService(dataPath string, docRepo *document.Repository, docStorage *document.Storage) *Service {
	return &Service{
		dataPath:   dataPath,
		docRepo:    docRepo,
		docStorage: docStorage,
	}
}

// init åˆå§‹åŒ–å†…éƒ¨ç»„ä»¶ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
func (s *Service) init() error {
	if s.embedder != nil {
		return nil // å·²åˆå§‹åŒ–
	}

	// åŠ è½½é…ç½®
	config, err := LoadConfig(s.dataPath)
	if err != nil {
		return err
	}

	// åˆ›å»º Embedding å®¢æˆ·ç«¯
	embedder, err := NewEmbeddingClient(config)
	if err != nil {
		return err
	}
	s.embedder = embedder

	// åˆ›å»ºå‘é‡å­˜å‚¨
	dbPath := filepath.Join(s.dataPath, "vectors.db")
	store, err := NewVectorStore(dbPath, embedder.Dimension())
	if err != nil {
		return err
	}
	s.store = store

	// åˆ›å»ºç´¢å¼•å™¨å’Œæœç´¢å™¨
	s.indexer = NewIndexer(store, embedder, s.docRepo, s.docStorage, s.dataPath)
	s.searcher = NewSearcher(store, embedder, s.docRepo)

	return nil
}

// IndexDocument ç´¢å¼•å•ä¸ªæ–‡æ¡£
func (s *Service) IndexDocument(docID string) error {
	if err := s.init(); err != nil {
		return err
	}
	return s.indexer.IndexDocument(docID)
}

// Search è¯­ä¹‰æœç´¢ï¼ˆChunk çº§åˆ«ï¼‰
func (s *Service) Search(query string, limit int) ([]SemanticSearchResult, error) {
	if err := s.init(); err != nil {
		return nil, err
	}
	return s.searcher.Search(query, limit)
}

// SearchDocuments æ–‡æ¡£çº§è¯­ä¹‰æœç´¢ï¼ˆèšåˆ chunksï¼‰
func (s *Service) SearchDocuments(query string, limit int) ([]DocumentSearchResult, error) {
	if err := s.init(); err != nil {
		return nil, err
	}
	return s.searcher.SearchDocuments(query, limit)
}

// ReindexAll é‡å»ºæ‰€æœ‰æ–‡æ¡£ç´¢å¼•
func (s *Service) ReindexAll() (int, error) {
	if err := s.init(); err != nil {
		return 0, err
	}
	return s.indexer.ReindexAll()
}

// SetContext è®¾ç½® Wails ä¸Šä¸‹æ–‡ï¼ˆç”¨äºŽå‘é€äº‹ä»¶ï¼‰
func (s *Service) SetContext(ctx context.Context) {
	s.ctx = ctx
}

// ReindexAllWithProgress é‡å»ºæ‰€æœ‰æ–‡æ¡£ç´¢å¼•ï¼ˆå¸¦è¿›åº¦å›žè°ƒï¼‰
func (s *Service) ReindexAllWithProgress(onProgress func(current, total int)) (int, error) {
	if err := s.init(); err != nil {
		return 0, err
	}
	return s.indexer.ReindexAllWithCallback(onProgress)
}

// DeleteDocument åˆ é™¤æ–‡æ¡£çš„æ‰€æœ‰å‘é‡ç´¢å¼•
func (s *Service) DeleteDocument(docID string) error {
	if err := s.init(); err != nil {
		return err
	}
	return s.store.DeleteByDocID(docID)
}

// GetIndexedCount èŽ·å–å·²ç´¢å¼•çš„æ–‡æ¡£æ•°é‡
func (s *Service) GetIndexedCount() (int, error) {
	if err := s.init(); err != nil {
		return 0, nil // åˆå§‹åŒ–å¤±è´¥ï¼Œè¿”å›ž 0
	}
	return s.store.GetIndexedDocCount()
}

// GetIndexedStats èŽ·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯ (æ–‡æ¡£æ•°, ä¹¦ç­¾æ•°, åµŒå…¥æ–‡ä»¶æ•°)
func (s *Service) GetIndexedStats() (int, int, int, error) {
	if err := s.init(); err != nil {
		return 0, 0, 0, nil // åˆå§‹åŒ–å¤±è´¥ï¼Œè¿”å›ž 0
	}
	return s.store.GetIndexedStats()
}

// Reinitialize é‡æ–°åˆå§‹åŒ–ï¼ˆé…ç½®å˜æ›´åŽè°ƒç”¨ï¼‰
// å¦‚æžœæ–°æ¨¡åž‹çš„ç»´åº¦ä¸Žæ—§æ¨¡åž‹ä¸åŒï¼Œä¼šè‡ªåŠ¨åˆ é™¤å‘é‡æ•°æ®åº“
func (s *Service) Reinitialize() error {
	// èŽ·å–æ—§çš„ç»´åº¦
	oldDimension := 0
	if s.embedder != nil {
		oldDimension = s.embedder.Dimension()
	}

	// å…³é—­æ—§çš„å­˜å‚¨
	if s.store != nil {
		if err := s.store.Close(); err != nil {
			fmt.Printf("âš ï¸ [RAG] Failed to close store: %v\n", err)
		}
	}

	// é‡ç½®æ‰€æœ‰ç»„ä»¶
	s.store = nil
	s.indexer = nil
	s.searcher = nil
	s.embedder = nil

	// åŠ è½½æ–°é…ç½®ï¼Œæ£€æŸ¥ç»´åº¦æ˜¯å¦å˜åŒ–
	config, err := LoadConfig(s.dataPath)
	if err != nil {
		return err
	}

	newEmbedder, err := NewEmbeddingClient(config)
	if err != nil {
		return err
	}
	newDimension := newEmbedder.Dimension()

	// æ£€æŸ¥ç»´åº¦æ˜¯å¦å˜åŒ–
	dimensionChanged := oldDimension > 0 && oldDimension != newDimension

	// å¦‚æžœç»´åº¦å˜åŒ–ï¼Œåˆ é™¤æ—§çš„å‘é‡æ•°æ®åº“
	if dimensionChanged {
		dbPath := filepath.Join(s.dataPath, "vectors.db")
		fmt.Printf("ðŸ”„ [RAG] Dimension changed (%d â†’ %d), removing old database...\n", oldDimension, newDimension)
		if err := os.Remove(dbPath); err != nil && !os.IsNotExist(err) {
			fmt.Printf("âš ï¸ [RAG] Failed to remove old database: %v\n", err)
		}
	}

	// é‡æ–°åˆå§‹åŒ–
	s.embedder = newEmbedder

	dbPath := filepath.Join(s.dataPath, "vectors.db")
	store, err := NewVectorStore(dbPath, newDimension)
	if err != nil {
		return err
	}
	s.store = store

	s.indexer = NewIndexer(store, s.embedder, s.docRepo, s.docStorage, s.dataPath)
	s.searcher = NewSearcher(store, s.embedder, s.docRepo)

	// å¦‚æžœç»´åº¦å˜åŒ–ï¼Œè‡ªåŠ¨è§¦å‘å…¨é‡é‡å»ºç´¢å¼•ï¼ˆåŒ…æ‹¬ bookmark å’Œ file å—ï¼‰
	if dimensionChanged {
		go func() {
			fmt.Println("ðŸ”„ [RAG] Starting automatic reindex due to dimension change...")
			if count, err := s.ReindexAll(); err != nil {
				fmt.Printf("âš ï¸ [RAG] ReindexAll failed: %v\n", err)
			} else {
				fmt.Printf("âœ… [RAG] Reindexed %d documents\n", count)
			}
			if extCount, err := s.ReindexExternalContent(); err != nil {
				fmt.Printf("âš ï¸ [RAG] ReindexExternalContent failed: %v\n", err)
			} else {
				fmt.Printf("âœ… [RAG] Reindexed %d external blocks (bookmarks + files)\n", extCount)
			}
		}()
	}

	return nil
}

// ReindexExternalContent é‡æ–°ç´¢å¼•æ‰€æœ‰ bookmark å’Œ file å—
// éåŽ†æ‰€æœ‰æ–‡æ¡£ï¼Œæå– bookmark/file å—ä¿¡æ¯ï¼Œç„¶åŽé‡æ–°æŠ“å–å’Œç´¢å¼•
func (s *Service) ReindexExternalContent() (int, error) {
	if err := s.init(); err != nil {
		return 0, err
	}

	// èŽ·å–æ‰€æœ‰æ–‡æ¡£
	index, err := s.docRepo.GetAll()
	if err != nil {
		return 0, fmt.Errorf("failed to get documents: %w", err)
	}

	totalCount := 0
	for _, doc := range index.Documents {
		// åŠ è½½æ–‡æ¡£å†…å®¹
		content, err := s.docStorage.Load(doc.ID)
		if err != nil {
			fmt.Printf("âš ï¸ [RAG] Failed to load document %s: %v\n", doc.ID, err)
			continue
		}

		// æå–å¤–éƒ¨å—ä¿¡æ¯
		externalIDs := ExtractExternalBlockIDs([]byte(content))

		// é‡æ–°ç´¢å¼• bookmark å—
		for _, bookmark := range externalIDs.BookmarkBlocks {
			if bookmark.URL == "" {
				continue
			}
			if err := s.IndexBookmarkContent(bookmark.URL, doc.ID, bookmark.BlockID); err != nil {
				fmt.Printf("âš ï¸ [RAG] Failed to reindex bookmark %s: %v\n", bookmark.BlockID, err)
			} else {
				totalCount++
				fmt.Printf("âœ… [RAG] Reindexed bookmark: %s\n", bookmark.URL)
			}
		}

		// é‡æ–°ç´¢å¼• file å—
		for _, file := range externalIDs.FileBlocks {
			if file.FilePath == "" {
				continue
			}
			if err := s.IndexFileContent(file.FilePath, doc.ID, file.BlockID); err != nil {
				fmt.Printf("âš ï¸ [RAG] Failed to reindex file %s: %v\n", file.BlockID, err)
			} else {
				totalCount++
				fmt.Printf("âœ… [RAG] Reindexed file: %s\n", file.FilePath)
			}
		}
	}

	return totalCount, nil
}

// ReindexExternalContentWithProgress é‡æ–°ç´¢å¼•æ‰€æœ‰ bookmark å’Œ file å—ï¼ˆå¸¦è¿›åº¦å›žè°ƒï¼‰
func (s *Service) ReindexExternalContentWithProgress(onProgress func(current, total int)) (int, error) {
	if err := s.init(); err != nil {
		return 0, err
	}

	// èŽ·å–æ‰€æœ‰æ–‡æ¡£å¹¶è®¡ç®—å¤–éƒ¨å—æ€»æ•°
	index, err := s.docRepo.GetAll()
	if err != nil {
		return 0, fmt.Errorf("failed to get documents: %w", err)
	}

	// å…ˆç»Ÿè®¡æ€»æ•°
	var allExternalBlocks []struct {
		docID    string
		bookmark *BookmarkBlockInfo
		file     *FileBlockInfo
	}

	for _, doc := range index.Documents {
		content, err := s.docStorage.Load(doc.ID)
		if err != nil {
			continue
		}
		externalIDs := ExtractExternalBlockIDs([]byte(content))
		for i := range externalIDs.BookmarkBlocks {
			if externalIDs.BookmarkBlocks[i].URL != "" {
				allExternalBlocks = append(allExternalBlocks, struct {
					docID    string
					bookmark *BookmarkBlockInfo
					file     *FileBlockInfo
				}{docID: doc.ID, bookmark: &externalIDs.BookmarkBlocks[i]})
			}
		}
		for i := range externalIDs.FileBlocks {
			if externalIDs.FileBlocks[i].FilePath != "" {
				allExternalBlocks = append(allExternalBlocks, struct {
					docID    string
					bookmark *BookmarkBlockInfo
					file     *FileBlockInfo
				}{docID: doc.ID, file: &externalIDs.FileBlocks[i]})
			}
		}
	}

	total := len(allExternalBlocks)
	if total == 0 {
		return 0, nil
	}

	successCount := 0
	for i, block := range allExternalBlocks {
		// å‘é€è¿›åº¦
		if onProgress != nil {
			onProgress(i+1, total)
		}

		if block.bookmark != nil {
			if err := s.IndexBookmarkContent(block.bookmark.URL, block.docID, block.bookmark.BlockID); err != nil {
				fmt.Printf("âš ï¸ [RAG] Failed to reindex bookmark %s: %v\n", block.bookmark.BlockID, err)
			} else {
				successCount++
				fmt.Printf("âœ… [RAG] Reindexed bookmark: %s\n", block.bookmark.URL)
			}
		} else if block.file != nil {
			if err := s.IndexFileContent(block.file.FilePath, block.docID, block.file.BlockID); err != nil {
				fmt.Printf("âš ï¸ [RAG] Failed to reindex file %s: %v\n", block.file.BlockID, err)
			} else {
				successCount++
				fmt.Printf("âœ… [RAG] Reindexed file: %s\n", block.file.FilePath)
			}
		}
	}

	return successCount, nil
}

// IndexBookmarkContent ç´¢å¼•ä¹¦ç­¾ç½‘é¡µå†…å®¹ï¼ˆåˆ†å—å­˜å‚¨ï¼‰
func (s *Service) IndexBookmarkContent(url, sourceDocID, blockID string) error {
	if err := s.init(); err != nil {
		return err
	}

	// 1. æŠ“å–ç½‘é¡µå†…å®¹
	content, err := opengraph.FetchContent(url)
	if err != nil {
		return fmt.Errorf("failed to fetch content: %w", err)
	}

	// 2. æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºç©º
	if content.TextContent == "" {
		return fmt.Errorf("no content extracted from URL")
	}

	// 3. æž„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
	headingContext := content.Title
	if content.SiteName != "" {
		headingContext = fmt.Sprintf("%s - %s", content.Title, content.SiteName)
	}

	// 4. ç”ŸæˆåŸºç¡€ ID
	baseID := fmt.Sprintf("%s_%s_bookmark", sourceDocID, blockID)

	// 5. åˆ é™¤è¯¥ bookmark block çš„æ—§ chunksï¼ˆä¿®å¤é‡æ–°ç´¢å¼•æ—¶çš„ä¸»é”®å†²çªï¼‰
	if err := s.store.DeleteBlocksByPrefix(baseID); err != nil {
		fmt.Printf("âš ï¸ [RAG] Failed to delete old bookmark chunks for %s: %v\n", baseID, err)
	}

	// 6. å¯¹å†…å®¹è¿›è¡Œåˆ†å—
	chunks := ChunkTextContent(content.TextContent, headingContext, baseID, s.indexer.chunkConfig)

	// å¦‚æžœåˆ†å—ç»“æžœä¸ºç©ºï¼Œåˆ›å»ºä¸€ä¸ªå•ç‹¬çš„å—
	if len(chunks) == 0 {
		chunks = []ExtractedBlock{{
			ID:             baseID,
			Type:           "bookmark",
			Content:        content.TextContent,
			HeadingContext: headingContext,
		}}
	}

	// è°ƒè¯•è¾“å‡º
	if debugChunks {
		fmt.Printf("\nðŸ”– [RAG] Indexing bookmark: %s\n", url)
		fmt.Printf("   Title: %s\n", content.Title)
		fmt.Printf("   Total chunks: %d\n", len(chunks))
		fmt.Println("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
		for i, chunk := range chunks {
			fmt.Printf("   [%d] ID: %s\n", i, chunk.ID)
			fmt.Printf("       Content (%4d chars): %s\n",
				len(chunk.Content), truncateContent(chunk.Content, 80))
		}
		fmt.Println("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
	}

	// 7. ä¸ºæ¯ä¸ª chunk ç”Ÿæˆ embedding å¹¶å­˜å‚¨
	successCount := 0
	failedCount := 0
	var lastError error
	for _, chunk := range chunks {
		if chunk.Content == "" {
			continue
		}

		embedding, err := s.embedder.Embed(chunk.Content)
		if err != nil {
			failedCount++
			lastError = err
			fmt.Printf("âš ï¸ [RAG] Failed to embed bookmark chunk %s: %v\n", chunk.ID, err)
			continue // è·³è¿‡å¤±è´¥çš„å—
		}

		contentHash := HashContent(chunk.Content)
		if err := s.store.Upsert(&BlockVector{
			ID:             chunk.ID,
			SourceBlockID:  blockID, // BookmarkBlock çš„ BlockNote IDï¼Œç”¨äºŽå®šä½
			DocID:          sourceDocID,
			Content:        chunk.Content,
			ContentHash:    contentHash,
			BlockType:      "bookmark",
			HeadingContext: chunk.HeadingContext,
			Embedding:      embedding,
		}); err != nil {
			fmt.Printf("âš ï¸ [RAG] Failed to upsert bookmark chunk %s: %v\n", chunk.ID, err)
			failedCount++
		} else {
			successCount++
		}
	}

	// å¦‚æžœæ‰€æœ‰ chunks éƒ½åµŒå…¥å¤±è´¥ï¼Œè¿”å›žé”™è¯¯
	if successCount == 0 && failedCount > 0 {
		return fmt.Errorf("embedding failed: %v", lastError)
	}

	return nil
}

// IndexFileContent ç´¢å¼•æ–‡ä»¶å†…å®¹ï¼ˆåˆ†å—å­˜å‚¨ï¼‰
func (s *Service) IndexFileContent(filePath, sourceDocID, blockID string) error {
	if err := s.init(); err != nil {
		return err
	}

	// 1. èŽ·å–å®Œæ•´æ–‡ä»¶è·¯å¾„
	fullPath := filepath.Join(s.dataPath, strings.TrimPrefix(filePath, "/"))

	// 2. æå–æ–‡æœ¬å†…å®¹
	textContent, err := fileextract.ExtractText(fullPath)
	if err != nil {
		return fmt.Errorf("failed to extract text: %w", err)
	}

	if textContent == "" {
		return fmt.Errorf("no text content extracted from file")
	}

	// 3. æž„å»ºä¸Šä¸‹æ–‡ï¼ˆä½¿ç”¨æ–‡ä»¶åï¼‰
	fileName := filepath.Base(fullPath)
	headingContext := fileName

	// 4. ç”ŸæˆåŸºç¡€ ID
	baseID := fmt.Sprintf("%s_%s_file", sourceDocID, blockID)

	// 5. åˆ é™¤è¯¥ file block çš„æ—§ chunksï¼ˆä¿®å¤é‡æ–°ç´¢å¼•æ—¶çš„ä¸»é”®å†²çªï¼‰
	if err := s.store.DeleteBlocksByPrefix(baseID); err != nil {
		fmt.Printf("âš ï¸ [RAG] Failed to delete old file chunks for %s: %v\n", baseID, err)
	}

	// 6. å¯¹å†…å®¹è¿›è¡Œåˆ†å—
	chunks := ChunkTextContent(textContent, headingContext, baseID, s.indexer.chunkConfig)

	// å¦‚æžœåˆ†å—ç»“æžœä¸ºç©ºï¼Œåˆ›å»ºä¸€ä¸ªå•ç‹¬çš„å—
	if len(chunks) == 0 {
		chunks = []ExtractedBlock{{
			ID:             baseID,
			Type:           "file",
			Content:        textContent,
			HeadingContext: headingContext,
		}}
	}

	// è°ƒè¯•è¾“å‡º
	if debugChunks {
		fmt.Printf("\nðŸ“„ [RAG] Indexing file: %s\n", fileName)
		fmt.Printf("   Total chunks: %d\n", len(chunks))
		fmt.Println("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
		for i, chunk := range chunks {
			fmt.Printf("   [%d] ID: %s\n", i, chunk.ID)
			fmt.Printf("       Content (%4d chars): %s\n",
				len(chunk.Content), truncateContent(chunk.Content, 80))
		}
		fmt.Println("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
	}

	// 7. ä¸ºæ¯ä¸ª chunk ç”Ÿæˆ embedding å¹¶å­˜å‚¨
	successCount := 0
	failedCount := 0
	var lastError error
	for _, chunk := range chunks {
		if chunk.Content == "" {
			continue
		}

		embedding, err := s.embedder.Embed(chunk.Content)
		if err != nil {
			failedCount++
			lastError = err
			fmt.Printf("âš ï¸ [RAG] Failed to embed file chunk %s: %v\n", chunk.ID, err)
			continue // è·³è¿‡å¤±è´¥çš„å—
		}

		contentHash := HashContent(chunk.Content)
		if err := s.store.Upsert(&BlockVector{
			ID:             chunk.ID,
			SourceBlockID:  blockID, // FileBlock çš„ BlockNote IDï¼Œç”¨äºŽå®šä½
			DocID:          sourceDocID,
			Content:        chunk.Content,
			ContentHash:    contentHash,
			BlockType:      "file",
			HeadingContext: chunk.HeadingContext,
			FilePath:       filePath, // å­˜å‚¨æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºŽåˆ é™¤æ—¶æ¸…ç†ç‰©ç†æ–‡ä»¶
			Embedding:      embedding,
		}); err != nil {
			fmt.Printf("âŒ [RAG] Failed to upsert file chunk %s: %v\n", chunk.ID, err)
			failedCount++
		} else {
			successCount++
			if debugChunks {
				fmt.Printf("âœ… [RAG] Stored file chunk: %s\n", chunk.ID)
			}
		}
	}

	// å¦‚æžœæ‰€æœ‰ chunks éƒ½åµŒå…¥å¤±è´¥ï¼Œè¿”å›žé”™è¯¯
	if successCount == 0 && failedCount > 0 {
		return fmt.Errorf("embedding failed: %v", lastError)
	}

	return nil
}
