package rag

import (
	"fmt"
	"os"
	"path/filepath"

	"notion-lite/internal/document"
	"notion-lite/internal/opengraph"
)

// Service RAG æœåŠ¡ç»Ÿä¸€å…¥å£
type Service struct {
	dataPath   string
	store      *VectorStore
	indexer    *Indexer
	searcher   *Searcher
	embedder   EmbeddingClient
	docRepo    *document.Repository
	docStorage *document.Storage
}

// NewService åˆ›å»º RAG æœåŠ¡
func NewService(dataPath string, docRepo *document.Repository, docStorage *document.Storage) *Service {
	return &Service{
		dataPath:   dataPath,
		docRepo:    docRepo,
		docStorage: docStorage,
	}
}

// init åˆå§‹åŒ–å†…éƒ¨ç»„ä»¶ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
func (s *Service) init() error {
	if s.embedder != nil {
		return nil // å·²åˆå§‹åŒ–
	}

	// åŠ è½½é…ç½®
	config, err := LoadConfig(s.dataPath)
	if err != nil {
		return err
	}

	// åˆ›å»º Embedding å®¢æˆ·ç«¯
	embedder, err := NewEmbeddingClient(config)
	if err != nil {
		return err
	}
	s.embedder = embedder

	// åˆ›å»ºå‘é‡å­˜å‚¨
	dbPath := filepath.Join(s.dataPath, "vectors.db")
	store, err := NewVectorStore(dbPath, embedder.Dimension())
	if err != nil {
		return err
	}
	s.store = store

	// åˆ›å»ºç´¢å¼•å™¨å’Œæœç´¢å™¨
	s.indexer = NewIndexer(store, embedder, s.docRepo, s.docStorage)
	s.searcher = NewSearcher(store, embedder, s.docRepo)

	return nil
}

// IndexDocument ç´¢å¼•å•ä¸ªæ–‡æ¡£
func (s *Service) IndexDocument(docID string) error {
	if err := s.init(); err != nil {
		return err
	}
	return s.indexer.IndexDocument(docID)
}

// Search è¯­ä¹‰æœç´¢ï¼ˆChunk çº§åˆ«ï¼‰
func (s *Service) Search(query string, limit int) ([]SemanticSearchResult, error) {
	if err := s.init(); err != nil {
		return nil, err
	}
	return s.searcher.Search(query, limit)
}

// SearchDocuments æ–‡æ¡£çº§è¯­ä¹‰æœç´¢ï¼ˆèšåˆ chunksï¼‰
func (s *Service) SearchDocuments(query string, limit int) ([]DocumentSearchResult, error) {
	if err := s.init(); err != nil {
		return nil, err
	}
	return s.searcher.SearchDocuments(query, limit)
}

// ReindexAll é‡å»ºæ‰€æœ‰æ–‡æ¡£ç´¢å¼•
func (s *Service) ReindexAll() (int, error) {
	if err := s.init(); err != nil {
		return 0, err
	}
	return s.indexer.ReindexAll()
}

// DeleteDocument åˆ é™¤æ–‡æ¡£çš„æ‰€æœ‰å‘é‡ç´¢å¼•
func (s *Service) DeleteDocument(docID string) error {
	if err := s.init(); err != nil {
		return err
	}
	return s.store.DeleteByDocID(docID)
}

// GetIndexedCount è·å–å·²ç´¢å¼•çš„æ–‡æ¡£æ•°é‡
func (s *Service) GetIndexedCount() (int, error) {
	if s.store == nil {
		dbPath := filepath.Join(s.dataPath, "vectors.db")
		store, err := NewVectorStore(dbPath, 768) // é»˜è®¤ç»´åº¦
		if err != nil {
			return 0, nil // æ•°æ®åº“ä¸å­˜åœ¨ï¼Œè¿”å› 0
		}
		s.store = store
	}
	return s.store.GetIndexedDocCount()
}

// GetIndexedStats è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
func (s *Service) GetIndexedStats() (int, int, error) {
	if s.store == nil {
		dbPath := filepath.Join(s.dataPath, "vectors.db")
		store, err := NewVectorStore(dbPath, 768) // é»˜è®¤ç»´åº¦
		if err != nil {
			return 0, 0, nil // æ•°æ®åº“ä¸å­˜åœ¨ï¼Œè¿”å› 0
		}
		s.store = store
	}
	return s.store.GetIndexedStats()
}

// Reinitialize é‡æ–°åˆå§‹åŒ–ï¼ˆé…ç½®å˜æ›´åè°ƒç”¨ï¼‰
// å¦‚æœæ–°æ¨¡å‹çš„ç»´åº¦ä¸æ—§æ¨¡å‹ä¸åŒï¼Œä¼šè‡ªåŠ¨åˆ é™¤å‘é‡æ•°æ®åº“
func (s *Service) Reinitialize() error {
	// è·å–æ—§çš„ç»´åº¦
	oldDimension := 0
	if s.embedder != nil {
		oldDimension = s.embedder.Dimension()
	}

	// å…³é—­æ—§çš„å­˜å‚¨
	if s.store != nil {
		s.store.Close()
	}

	// é‡ç½®æ‰€æœ‰ç»„ä»¶
	s.store = nil
	s.indexer = nil
	s.searcher = nil
	s.embedder = nil

	// åŠ è½½æ–°é…ç½®ï¼Œæ£€æŸ¥ç»´åº¦æ˜¯å¦å˜åŒ–
	config, err := LoadConfig(s.dataPath)
	if err != nil {
		return err
	}

	newEmbedder, err := NewEmbeddingClient(config)
	if err != nil {
		return err
	}
	newDimension := newEmbedder.Dimension()

	// å¦‚æœç»´åº¦å˜åŒ–ï¼Œåˆ é™¤æ—§çš„å‘é‡æ•°æ®åº“
	if oldDimension > 0 && oldDimension != newDimension {
		dbPath := filepath.Join(s.dataPath, "vectors.db")
		os.Remove(dbPath) // å¿½ç•¥é”™è¯¯ï¼Œå¯èƒ½æ–‡ä»¶ä¸å­˜åœ¨
	}

	// é‡æ–°åˆå§‹åŒ–
	s.embedder = newEmbedder

	dbPath := filepath.Join(s.dataPath, "vectors.db")
	store, err := NewVectorStore(dbPath, newDimension)
	if err != nil {
		return err
	}
	s.store = store

	s.indexer = NewIndexer(store, s.embedder, s.docRepo, s.docStorage)
	s.searcher = NewSearcher(store, s.embedder, s.docRepo)

	return nil
}

// IndexBookmarkContent ç´¢å¼•ä¹¦ç­¾ç½‘é¡µå†…å®¹ï¼ˆåˆ†å—å­˜å‚¨ï¼‰
func (s *Service) IndexBookmarkContent(url, sourceDocID, blockID string) error {
	if err := s.init(); err != nil {
		return err
	}

	// 1. æŠ“å–ç½‘é¡µå†…å®¹
	content, err := opengraph.FetchContent(url)
	if err != nil {
		return fmt.Errorf("failed to fetch content: %w", err)
	}

	// 2. æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºç©º
	if content.TextContent == "" {
		return fmt.Errorf("no content extracted from URL")
	}

	// 3. æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
	headingContext := content.Title
	if content.SiteName != "" {
		headingContext = fmt.Sprintf("%s - %s", content.Title, content.SiteName)
	}

	// 4. ç”ŸæˆåŸºç¡€ ID
	baseID := fmt.Sprintf("%s_%s_bookmark", sourceDocID, blockID)

	// 5. å¯¹å†…å®¹è¿›è¡Œåˆ†å—
	chunks := ChunkTextContent(content.TextContent, headingContext, baseID, s.indexer.chunkConfig)

	// å¦‚æœåˆ†å—ç»“æœä¸ºç©ºï¼Œåˆ›å»ºä¸€ä¸ªå•ç‹¬çš„å—
	if len(chunks) == 0 {
		chunks = []ExtractedBlock{{
			ID:             baseID,
			Type:           "bookmark",
			Content:        content.TextContent,
			HeadingContext: headingContext,
		}}
	}

	// è°ƒè¯•è¾“å‡º
	if debugChunks {
		fmt.Printf("\nğŸ”– [RAG] Indexing bookmark: %s\n", url)
		fmt.Printf("   Title: %s\n", content.Title)
		fmt.Printf("   Total chunks: %d\n", len(chunks))
		fmt.Println("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
		for i, chunk := range chunks {
			fmt.Printf("   [%d] ID: %s\n", i, chunk.ID)
			fmt.Printf("       Content (%4d chars): %s\n",
				len(chunk.Content), truncateContent(chunk.Content, 80))
		}
		fmt.Println("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
	}

	// 6. ä¸ºæ¯ä¸ª chunk ç”Ÿæˆ embedding å¹¶å­˜å‚¨
	for _, chunk := range chunks {
		if chunk.Content == "" {
			continue
		}

		embedding, err := s.embedder.Embed(chunk.Content)
		if err != nil {
			continue // è·³è¿‡å¤±è´¥çš„å—
		}

		contentHash := HashContent(chunk.Content)
		s.store.Upsert(&BlockVector{
			ID:             chunk.ID,
			DocID:          sourceDocID,
			Content:        chunk.Content,
			ContentHash:    contentHash,
			BlockType:      "bookmark",
			HeadingContext: chunk.HeadingContext,
			Embedding:      embedding,
		})
	}

	return nil
}
